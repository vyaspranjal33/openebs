# test-disk-failure.yaml
# Author: nsathyaseelan
# Description: Include resiliency test suite in OpenEBS e2e (Test: disk failure)

###############################################################################################
# Prerequisities:
# 1. Add a Disk, Format and Mount the disk in kubernetes node.
# 2. Get the UUID of the Additional disk.
# 3. Run the Playbook with the UUID of the additional Disk as a external variable.
#
#Test Steps:
#1. Copy/Gather Test artifacts to Kubemaster.
#2. Create Storage pool Path to the additional disk in Nodes.
#3. Create Storage classes with the newly added storage pool path.
#4. Deploy Percona application.
#5. Failing the additional disk in node.
#6. Verify replica pod status.
#7. List the scsi hosts and Rescan the disk.
#8. Get the disk name by using UUID.
#9. Remount the Disk with the same Mount Point.
#10 Verify replica pod is scheduled and Running .
#11 Perform cleanup of test artifacts.
###############################################################################################

- hosts: localhost

  vars_files:
    - test-disk-failure-vars.yml

  tasks:
   - block:

       - name: 1) Install the prerequisites
         include: test-disk-failure-prereq.yml

       - name: 2) Check status of maya-api server
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
           ns: openebs
           lkey: name
           lvalue: maya-apiserver

       - name: 3) Get the disk name by using UUID
         shell: >
              blkid | grep {{ uuid }} | awk '{print $1}'
              | cut -d "/" -f3 | cut -d ":" -f1
         args:
           executable: /bin/bash
         become: true
         register: result_disk
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 3a) Get the Mount point of the disk using UUID
         shell: lsblk -f | grep {{ uuid }} | awk '{print $4}'
         args:
           executable: /bin/bash
         become: true
         register: result_mountpoint
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 4) Obtaining the storage pool creation yaml file
         get_url:
           url: "{{ storage_pool_link }}"
           dest: "{{ result_kube_home.stdout }}"
         register: result_storagepool
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 4a) Replace storage pool name with test parameters
         include_tasks: "{{utils_path}}/regex_task.yml"
         vars:
           path: "{{ result_kube_home.stdout }}/storage-pool-path_creation.yml"
           regex1: "{{replace_item}}"
           regex2: "{{replace_with}}"

       - name: 5) Create the Namespace
         include_tasks: "{{utils_path}}/namespace_task.yml"
         vars:
           status: create
           ns: "{{ namespace }}"


       - name: 5)Create the storage pool path
         shell: source ~/.profile; kubectl apply -f storage-pool-path_creation.yml  -n {{ namespace }}
         args:
           executable: /bin/bash
         register: result_sp_path
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 5a) Obtaining storage classes yaml
         shell: source ~/.profile; kubectl get sc openebs-percona -o yaml > "{{ create_sc }}"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 5b) Replace storage class file with test parameters
         replace:
           path: "{{ result_kube_home.stdout }}/{{ create_sc }}"
           regexp: 'openebs.io/storage-pool: default'
           replace: 'openebs.io/storage-pool: disk'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name:  5c) Delete the existing storage class and create new one
         shell: source ~/.profile; kubectl delete sc openebs-percona; kubectl apply -f "{{ create_sc }}"
         args:
           executable: /bin/bash
         register: result_sc_out
         until: "'created' in result_sc_out.stdout"
         delay: 10
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 6) Obtain the sql test script file
         get_url:
           url: "{{ sql_test_link }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 6a) Create the configmap test
         shell: source ~/.profile; kubectl create configmap sqltest --from-file=sql-test.sh  -n {{ namespace }}
         args:
           executable: /bin/bash
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 7) Obtaining the percona liveliness application yaml
         get_url:
           url: "{{ percona_link }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 7a) Create percona deployment with OpenEBS storage
         include_tasks: "{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ percona_files.0 }}"
           ns: "{{ namespace }}"

       - name: 7c) Check whether percona application is running
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:           
           ns: "{{ namespace }}"
           lkey: name
           lvalue: percona

       - name: 7d) Check Replica pod status
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
           ns: "{{ namespace }}"
           lkey: openebs.io/replica
           lvalue: jiva-replica           

       - name: 8) Obtaining the replica pods running on specified node
         shell: >
            source ~/.profile; kubectl get pods -o wide -n {{ namespace }}
            | grep newos2 | grep rep | awk '{ print $1 }'
         args:
           executable: /bin/bash
         become: true
         register: result_path
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 8a) Check Replica pods running on specified storage pool path
         shell: >
           source ~/.profile; kubectl describe pod  -n {{ namespace }} "{{ result_path.stdout }}"
           | grep "{{ result_mountpoint.stdout }}" | awk '{print $2}'
         args:
           executable: /bin/bash
         become: true
         register: result_rep_path
         failed_when: 'result_rep_path.rc!=0'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 9) Failing the additional disk
         shell: echo 1 > /sys/block/{{ result_disk.stdout }}/device/delete
         args:
           executable: /bin/bash
         become: true
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 9a) Check the Disk is failing
         shell: lsblk | grep {{ result_disk.stdout }}
         args:
           executable: /bin/bash
         become: true
         register: result_disk_check
         failed_when: 'result_disk_check.rc!=1'
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 10) Check the replica pod on the specified node
         shell: >
          source ~/.profile; kubectl get pods -o wide -n {{ namespace }}
          | grep "{{ groups['kubernetes-kubeminions'].0 }}" | grep rep | awk '{ print $1 }'
         args:
           executable: /bin/bash
         register: result_rep
         become: true
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         
           ######################################################################## 
           # After failing the underlying Disk it is expected that the replica    #
           # will crash. But the actual behaviour replica is still in Running     #
           # state. Refer : https://github.com/openebs/openebs/issues/1387        #
           ########################################################################

       - name: 10a) Delete the replica pod on the specified node
         shell: source ~/.profile; kubectl delete pod -n {{ namespace }} {{ result_rep.stdout }}
         args:
           executable: /bin/bash
         become: true
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: Wait for 120s to ensure liveness check starts
         wait_for:
           timeout: 120

       - name: 10b) Check the replica pod status scheduled on the specified node
         shell: >
           source ~/.profile; kubectl get pods -o wide -n {{ namespace }}
           | grep "{{ groups['kubernetes-kubeminions'].0 }}" | grep rep | awk '{ print $3 }'
         args:
           executable: /bin/bash
         register: result_rep_status
         failed_when: "'Running' in result_rep_status.stdout"
         become: true
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 10c) Check the percona application pod status
         shell: >
           source ~/.profile; kubectl get pods -o wide -n {{ namespace }} | grep percona | awk '{ print $3 }'
         args:
           executable: /bin/bash
         register: result_pod_status
         become: true
         failed_when: "'Running' not in  result_pod_status.stdout"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: 11) List the available scsi host
         shell: ls /sys/class/scsi_host/
         become: true
         register: result_host
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 11a) Rescan the disk
         shell: echo "- - -" > /sys/class/scsi_host/{{ item }}/scan
         args:
           executable: /bin/bash
         become: true
         with_items: "{{ result_host.stdout_lines }}"
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 12) Get the disk name by using UUID
         shell: >
            blkid | grep {{ uuid }} | awk '{print $1}'
            | cut -d "/" -f3 | cut -d ":" -f1
         args:
           executable: /bin/bash
         become: true
         register: result_scan_disk
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"

       - name: 12a) Remount the disk
         shell: mount /dev/{{ result_scan_disk.stdout }} {{ result_mountpoint.stdout }}
         args:
           executable: /bin/bash
         become: true
         delegate_to: "{{ groups['kubernetes-kubeminions'].0 }}"
         tags:
          - skip_ansible_lint

       - name: 13) Check Replica pod status
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
           ns: "{{ namespace }}"
           lkey: openebs.io/replica
           lvalue: jiva-replica

       - name: 14) Get Controller SVC IP
         shell: >
            source ~/.profile;
            kubectl get svc -n {{ namespace }} | grep ctrl | awk {'print $3'}
         args:
           executable: /bin/bash
         register: result_svc
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         changed_when: true

       - name: 14a) Check if replica is rebuilt
         shell: curl GET http://{{ result_svc.stdout }}:9501/v1/replicas | grep createTypes | jq -r '.data[].mode' | grep 'RW'
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "result.stdout_lines | length  == 3"
         delay: 60
         retries: 10
         changed_when: true
         tags:
          - skip_ansible_lint

       - name: Test Passed
         set_fact:
            flag: "Test Passed"

       - name: Set Status
         set_fact:
           status: "good"

     rescue:
        - name: Test Failed
          set_fact:
            flag: "Test Failed"

        - name: Set Status
          set_fact:
            status: "danger"

     always:
       - block:

           - name: 15) Cleaning up the test artifacts
             include: test-disk-failure-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - include_tasks: "{{utils_path}}/namespace_task.yml"
             vars:
               status: delete
               ns: "{{ namespace }}"

           - name: Send slack notification
             slack:
               token: "{{ lookup('env','SLACK_TOKEN') }}"
               msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }}, {{ cflag }}'
               color: "{{status}}"
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')
